{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ASUM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import re\n",
    "import pandas as pd\n",
    "from collections import Counter\n",
    "from wordcloud import WordCloud\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from reviews.config import processed_data_dir, asum_input_dir, asum_output_dir\n",
    "from reviews.preprocess import preprocess\n",
    "from reviews.models import asum"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# read data\n",
    "reviews_df = pd.read_json(processed_data_dir / \"reviews_digital_cameras.json.gz\", orient=\"records\")\n",
    "reviews_df.dropna(inplace=True)\n",
    "reviews_df.drop_duplicates(inplace=True)\n",
    "\n",
    "reviews_df = reviews_df.sample(100) # subset\n",
    "\n",
    "reviews_df.reset_index(inplace=True, drop=True)\n",
    "reviews_df.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# apply preprocessing\n",
    "reviews_df[\"tokens\"] = reviews_df[\"text\"].apply(lambda x : preprocess(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokens = [word for doc in reviews_df[\"tokens\"] for sent in doc for word in sent]\n",
    "c = Counter(tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# generate the word cloud\n",
    "w = WordCloud(collocations = False, prefer_horizontal=0.6, width=600, height=400, background_color=\"white\").fit_words(c)\n",
    "w.to_image()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vocabulary = dict((e, i) for i, e in enumerate(set(tokens)))\n",
    "print(len(vocabulary))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generate data for ASUM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(asum_input_dir / \"WordList.txt\", \"w\") as f:\n",
    "  for word in vocabulary:\n",
    "    f.write(f\"{word}\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sentence_list = asum_input_dir / \"BagOfSentences.txt\"\n",
    "\n",
    "if os.path.exists(sentence_list):\n",
    "  os.remove(sentence_list)\n",
    "\n",
    "with open(sentence_list, \"a\") as f:\n",
    "  for doc in reviews_df[\"tokens\"]:\n",
    "    f.write(f\"{len(doc)}\\n\")\n",
    "    for tokens in doc:\n",
    "      indexes = \" \".join([str(vocabulary[t]) for t in tokens])\n",
    "      f.write(f\"{indexes}\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "asum(\"0.1\", \"0.001/0.1/0\", \"1/1\", \"5\", iterations=\"500\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Print Values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(\n",
    "  asum_output_dir / \"STO2-T5-S2(2)-A0.1-B0.001,0.1,0.0-G1.0,1.0-I100-ProbWords.csv\"\n",
    ")\n",
    "\n",
    "def map_topic_sentiment(column_name):\n",
    "  if (\"Unnamed\" in column_name):\n",
    "    return None\n",
    "\n",
    "  values = column_name.split(\"-\")\n",
    "  return {\"sentiment\": int(values[0][1]), \"topic\": int(values[1][1]), \"colname\": column_name }\n",
    "\n",
    "columns = df.columns.map(map_topic_sentiment)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_word_weight(item):\n",
    "  match = re.match(\"(\\w+)\\s+\\((.*?)\\)\", item)\n",
    "  word, weight = match.groups()\n",
    "  return word, float(weight)\n",
    "\n",
    "n_top_words = 10\n",
    "\n",
    "fig, axes = plt.subplots(2, n_top_words, figsize=(15, 6), sharex=True)\n",
    "axes = axes.flatten()\n",
    "\n",
    "for idx, item in enumerate(columns):\n",
    "  if item is None:\n",
    "    continue\n",
    "  \n",
    "  ax = axes[idx]\n",
    "  words = df[item[\"colname\"]][:n_top_words]\n",
    "  weights = list(map(list, zip(*words.map(get_word_weight))))\n",
    "\n",
    "  weights = pd.DataFrame({\"names\": weights[0], \"weights\": weights[1]})\n",
    "  weights.sort_values(\"weights\", inplace=True)\n",
    "  \n",
    "  ax.barh(weights[\"names\"], weights[\"weights\"], height=0.5)\n",
    "  \n",
    "  if idx > n_top_words - 1:\n",
    "    ax.set_xlabel(f\"Topic {item['topic'] + 1}\")\n",
    "\n",
    "  if idx == 0 or idx == n_top_words:\n",
    "    ax.set_ylabel(\"Positive\" if item['sentiment'] == 0 else \"Negative\")\n",
    "fig.tight_layout()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import seaborn as sns\n",
    "\n",
    "pi_df = pd.read_csv(\n",
    "  asum_output_dir / \"STO2-T5-S2(2)-A0.1-B0.001,0.1,0.0-G1.0,1.0-I500-Pi.csv\",\n",
    "  header=None,\n",
    ")\n",
    "\n",
    "doc_sentiment = pi_df.idxmax(axis=1).map(lambda x: \"positive\" if x == 0 else \"negative\")\n",
    "doc_sentiment.loc[pi_df[0] == 0.5] = \"neutral\"\n",
    "\n",
    "reviews_df[\"sentiment\"] = doc_sentiment\n",
    "\n",
    "sns.histplot(x = doc_sentiment)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "theta_df = pd.read_csv(\n",
    "  asum_output_dir / \"STO2-T5-S2(2)-A0.1-B0.001,0.1,0.0-G1.0,1.0-I500-Theta.csv\"\n",
    ")\n",
    "theta_df.drop(columns=[\"Unnamed: 10\"], inplace=True)\n",
    "\n",
    "mask = theta_df >= 0.3\n",
    "mask = mask.apply(lambda x : list(mask.columns[x]), axis=1)\n",
    "\n",
    "# theta_df.loc[mask.transform(lambda x : len(x)) == 0]\n",
    "\n",
    "reviews_df[\"topics\"] = mask.apply(lambda x : list(map(map_topic_sentiment, x)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "reviews_df.to_json(asum_output_dir / \"topics.json\", orient=\"records\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.10.4 ('venv': venv)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  },
  "vscode": {
   "interpreter": {
    "hash": "b3de614af504821bb8883dc4474be6409a07bf1012b2a3a30887ac305417f630"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
