{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# JST"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO: Pandarallel will run on 8 workers.\n",
      "INFO: Pandarallel will use Memory file system to transfer data between the main process and workers.\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "import time\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "from itertools import chain\n",
    "from pandarallel import pandarallel\n",
    "from wordcloud import WordCloud\n",
    "from collections import Counter\n",
    "from sklearn.metrics import f1_score\n",
    "\n",
    "from reviews.config import processed_data_dir, data_dir, out_dir\n",
    "from reviews.utils import flat_sentence_tokens\n",
    "\n",
    "\n",
    "pandarallel.initialize(progress_bar=False, nb_workers=8)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generate Input Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "outputs": [],
   "source": [
    "NORM = \"stemming\"\n",
    "FIELD = \"text\""
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "reviews_df = pd.read_json(\n",
    "    processed_data_dir / f\"reviews_{FIELD}_{NORM}.json.gz\", orient=\"records\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# load tokens\n",
    "\n",
    "too_long = reviews_df[\"tokens\"].apply(lambda x: len(list(chain.from_iterable(x))) > 50)\n",
    "reviews_df.loc[too_long, \"tokens\"] = \"[]\"\n",
    "\n",
    "print(f\"Too Long Reviews: {too_long.sum() / len(reviews_df) * 100:.2f}%\")"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "outputs": [],
   "source": [
    "# docs\n",
    "sentences = list(reviews_df[\"tokens\"].apply(lambda r: \" \".join(chain.from_iterable(r))))\n",
    "docs = [f\"d{i} {x}\" for i, x in enumerate(sentences)]\n",
    "\n",
    "with open(data_dir / \"jst\" / \"docs.dat\", \"w\") as f:\n",
    "    f.write(\"\\n\".join(docs))"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# vocabulary\n",
    "tokens = flat_sentence_tokens(reviews_df[\"tokens\"])\n",
    "vocabs = [f\"{k} {v}\" for (k, v) in dict(Counter(tokens)).items()]\n",
    "\n",
    "with open(data_dir / \"jst\" / \"wordmap.txt\", \"w\") as f:\n",
    "    f.write(\"\\n\".join(vocabs))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "outputs": [],
   "source": [
    "# sentiment words\n",
    "with open(data_dir / \"sentiwords.json\", \"r\") as f:\n",
    "    senti_words = json.load(f)\n",
    "\n",
    "    normalized = senti_words[NORM]\n",
    "\n",
    "    pos_words = [f\"{w} 1 0\" for w in normalized[\"positive\"]]\n",
    "    neg_words = [f\"{w} 0 1\" for w in normalized[\"negative\"]]\n",
    "\n",
    "with open(data_dir / \"jst\" / \"sentiwords.txt\", \"w\") as f:\n",
    "    f.write(\"\\n\".join(pos_words + neg_words))"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Run"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "is_executing": true
    }
   },
   "outputs": [],
   "source": [
    "# JST execution\n",
    "from reviews.config import data_dir, bin_dir, out_dir\n",
    "from reviews.models import JST\n",
    "\n",
    "n_topics = 10\n",
    "n_runs = 10\n",
    "iterations = 1000\n",
    "\n",
    "alpha = -1\n",
    "beta = -1\n",
    "gamma = -1\n",
    "\n",
    "metrics = []\n",
    "for run in range(n_runs):\n",
    "    # train the model\n",
    "    model = JST(bin_dir, data_dir / \"jst\", out_dir / \"jst\")\n",
    "    start_time = time.time()\n",
    "    model.estimate(alpha, beta, gamma, n_topics, iterations)\n",
    "    print(f\"Run {run}: {(time.time() - start_time)}s\")\n",
    "\n",
    "    # compute metrics\n",
    "    pi = pd.read_csv(out_dir / \"jst\" / \"final.pi\", sep=\" \", header=None)\n",
    "    pi.drop([0, 1, 4], axis=1, inplace=True)\n",
    "    pi.columns = [\"S0\", \"S1\"]\n",
    "    pi_df = pi\n",
    "    doc_sentiment = pi_df.idxmax(axis=1).map(\n",
    "        lambda x: \"positive\" if x == \"S0\" else \"negative\"\n",
    "    )\n",
    "    reviews_df[\"sentiment\"] = doc_sentiment\n",
    "\n",
    "    gt = reviews_df[reviews_df[\"overall\"] != 3][\"overall\"]\n",
    "    pred = reviews_df[reviews_df[\"overall\"] != 3][\"sentiment\"]\n",
    "\n",
    "    y_true = gt.apply(lambda x: \"negative\" if x < 3 else \"positive\").astype(\"category\")\n",
    "    y_pred = pred.astype(\"category\")\n",
    "\n",
    "    metrics.append([\n",
    "        run, n_topics,\n",
    "        f1_score(y_true, y_pred, average=\"macro\"),\n",
    "        f1_score(y_true, y_pred, average=\"weighted\"),\n",
    "        f1_score(y_true, y_pred, average=\"micro\"),\n",
    "    ])\n",
    "\n",
    "metrics_df = pd.DataFrame(metrics, columns=['run', 'n_topics', 'f1_macro', 'f1_weighted', 'f1_micro'])\n",
    "metrics_df.to_csv(out_dir / 'jst' / f'metrics_jst_{n_topics}_{FIELD}_{NORM}.csv')\n",
    "metrics_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## prepare data for analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# columns name\n",
    "columns_name = []\n",
    "\n",
    "for x in range(2):\n",
    "    for y in range(n_topics):\n",
    "        columns_name.append(\"S\" + str(x) + \"-T\" + str(y))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# prepare phi file for analysis\n",
    "with open(out_dir / \"jst\" / \"final.phi\") as f:\n",
    "    lines = f.readlines()\n",
    "\n",
    "    phi = {}\n",
    "    tmp = \"\"\n",
    "    for i, line in enumerate(lines):\n",
    "        if i % 2 != 0:\n",
    "            phi[tmp] = [float(x.strip()) for x in line.split(\" \") if x.strip() != \"\"]\n",
    "        else:\n",
    "            tmp = line.strip()\n",
    "\n",
    "phi = pd.DataFrame(phi)\n",
    "phi.columns = columns_name\n",
    "phi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# prepare pi file for analysis\n",
    "pi = pd.read_csv(out_dir / \"jst\" / \"final.pi\", sep=\" \", header=None)\n",
    "pi.drop([0, 1, 4], axis=1, inplace=True)\n",
    "pi.columns = [\"S0\", \"S1\"]\n",
    "pi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(out_dir / \"jst\" / \"final.theta\") as f:\n",
    "    lines = f.readlines()\n",
    "\n",
    "    theta = []\n",
    "    tmp = []\n",
    "    for i, line in enumerate(lines):\n",
    "        if i % 3 != 0:\n",
    "            values = [float(x.strip()) for x in line.split(\" \") if x.strip() != \"\"]\n",
    "            tmp.extend(values)\n",
    "        else:\n",
    "            if tmp:\n",
    "                theta.append(tmp)\n",
    "                tmp = []\n",
    "\n",
    "theta = pd.DataFrame(theta, columns=columns_name)\n",
    "theta"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pi_df = pi\n",
    "doc_sentiment = pi_df.idxmax(axis=1).map(\n",
    "    lambda x: \"positive\" if x == \"S0\" else \"negative\"\n",
    ")\n",
    "doc_sentiment.loc[pi_df[\"S0\"] == 0.5] = \"neutral\"\n",
    "\n",
    "reviews_df[\"sentiment\"] = doc_sentiment\n",
    "\n",
    "fig, axes = plt.subplots(1, 2, figsize=(10, 3))\n",
    "sns.histplot(x=doc_sentiment, ax=axes[0])\n",
    "\n",
    "counts = doc_sentiment.value_counts()\n",
    "counts.plot(\n",
    "    ax=axes[1],\n",
    "    kind=\"pie\",\n",
    "    ylabel=\"sentiment\",\n",
    "    # colors=sns.color_palette(\"pastel\")[0:7],\n",
    "    autopct=\"%.0f%%\",\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "def get_word_indexes():\n",
    "    words = {}\n",
    "    with open(out_dir / \"jst\" / \"wordmap.txt\") as f:\n",
    "        for line in f.readlines()[1:]:\n",
    "            word_idx = line.split(\" \")\n",
    "            word = word_idx[0].strip()\n",
    "            idx = word_idx[1].strip()\n",
    "            words[idx] = word\n",
    "    return words\n",
    "\n",
    "\n",
    "words_map = get_word_indexes()"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "wc = WordCloud(height=400, width=800)\n",
    "wc.generate_from_frequencies(\n",
    "    dict(zip(list([words_map[str(idx)] for idx in phi.index]), phi[\"S1-T0\"].values))\n",
    ")\n",
    "wc.to_image()"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "def map_topic_sentiment(column_name):\n",
    "    if \"Unnamed\" in column_name:\n",
    "        return None\n",
    "\n",
    "    values = column_name.split(\"-\")\n",
    "    return {\n",
    "        \"sentiment\": int(values[0][1]),\n",
    "        \"topic\": int(values[1][1:]),\n",
    "        \"colname\": column_name,\n",
    "    }\n",
    "\n",
    "\n",
    "mask = theta >= 0.1\n",
    "mask = mask.apply(lambda x: list(mask.columns[x]), axis=1)\n",
    "\n",
    "reviews_df[\"topics\"] = mask.apply(lambda x: list(map(map_topic_sentiment, x)))"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "count = Counter()\n",
    "for x in reviews_df[\"topics\"].dropna().values:\n",
    "    topics = set([\"T\" + str(y[\"topic\"]) for y in x])\n",
    "    count.update(topics)\n",
    "\n",
    "topics_count = pd.DataFrame(count.items(), columns=[\"topic\", \"count\"])\n",
    "topics_count[\"topic\"] = topics_count[\"topic\"].astype(\"category\")\n",
    "\n",
    "order = topics_count.sort_values(by=\"count\", ascending=False).topic\n",
    "fig = plt.figure(figsize=(8, 5))\n",
    "ax = sns.barplot(\n",
    "    y=topics_count[\"topic\"], x=topics_count[\"count\"], order=order, orient=\"h\"\n",
    ")\n",
    "ax.set_title(\"Topics\")\n",
    "fig.tight_layout()"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "pos_count = Counter()\n",
    "neg_count = Counter()\n",
    "\n",
    "for x in reviews_df[\"topics\"].dropna().values:\n",
    "    pos_topics = set([\"T\" + str(st[\"topic\"]) for st in x if st[\"sentiment\"] == 0])\n",
    "    neg_topics = set([\"T\" + str(st[\"topic\"]) for st in x if st[\"sentiment\"] == 1])\n",
    "\n",
    "    pos_count.update(pos_topics)\n",
    "    neg_count.update(neg_topics)\n",
    "\n",
    "pos_df = pd.DataFrame(pos_count.items(), columns=[\"topic\", \"pos\"])\n",
    "neg_df = pd.DataFrame(neg_count.items(), columns=[\"topic\", \"neg\"])\n",
    "\n",
    "st_counts = pd.merge(pos_df, neg_df, on=\"topic\")\n",
    "st_counts[\"topic\"] = st_counts[\"topic\"].astype(\"category\")\n",
    "\n",
    "total = st_counts[\"pos\"] + st_counts[\"neg\"]\n",
    "st_counts[\"pos\"] = st_counts[\"pos\"] / total * 100\n",
    "st_counts[\"neg\"] = st_counts[\"neg\"] / total * 100\n",
    "\n",
    "st_counts.set_index(\"topic\", inplace=True)\n",
    "st_counts.sort_index(inplace=True)\n",
    "st_counts = st_counts.iloc[[int(o[1]) for o in order][::-1]]\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(10, 7))\n",
    "st_counts.plot(\n",
    "    kind=\"barh\", stacked=True, color=[\"red\", \"green\"], ax=ax, title=\"Topics Sentiment\"\n",
    ")\n",
    "fig.tight_layout()"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "reviews_df[\"true\"] = (\n",
    "    reviews_df[\"overall\"]\n",
    "    .apply(lambda x: \"negative\" if x < 3 else (\"neutral\" if x == 3 else \"positive\"))\n",
    "    .astype(\"category\")\n",
    ")"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "pd.crosstab(\n",
    "    reviews_df[\"true\"],\n",
    "    reviews_df[\"sentiment\"].astype(\"category\"),\n",
    ")"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "f1_score(reviews_df[\"true\"], reviews_df[\"sentiment\"], average=\"weighted\")"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.10.6 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "a3385c16acc5cf4675c171722a51dfd54bbb1cd274a594bf12acad6f6f9af2ca"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
